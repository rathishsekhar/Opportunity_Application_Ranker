# Opportunity Application Ranker - Under Construction
The project analyzes the textual data from job postings (opportunities) and applicant profiles to calculate similarity and rank applicants based on their similarity to a given job opportunity. As an exploratory project, three NLP approaches viz. word2vec, BERT based (uncased) and DistillBERT based (uncased) are employed. Further, the results of the models were subjected to an comparitive analysis to see which amongst these models can be used for scaling into a full fledged application. For the user's understanding, a locally available application, created using Streamlit library demonstrates the workings of all the three models. Later an another application is created to demonstrate the project using the most optimum model and this applcation is cloud deployed. The code and details for the optimized application will not be shared here in this document. To access the applcation code available on Github, click the following link:

https://github.com/rathishsekhar/Opportunity_Application_Matching_App

# Table of Contents
1. Project Overview
2. Motivation
3. Project Structure
4. Data 
5. Features
6. Prerequisites
8. Usage
9. Results
10. Limitations
11. Ethical Considerations
12. Contributing
13. License
14. Acknowledgements
15. Contact

# Project Overview
This project analyzes candidate data recorded by executive recruiters or headhunters. The projects aims to analyze typical job-candidate data to 
1. Identify suitable job matches for each candidate within the existing database of jobs
2. Calculate the compatibility between candidates versus the associated jobs through machine learning techniques. 
3. Create and demonstrate an user friendly application where new candidates profile can be inputted to receive the job that matches closest matching profile. 

Overall, the project aims to help recruiters efficiently align candidate profiles with relevant job opportunities. The insights generated by this project aim to streamline the matching process, reducing the manual effort required to match candidates to job roles. This approach could be extended to other applications, such as matching donors to applicants, mentors to mentees, or students to programs.

# Motivation
Matching assessed candidates with existing job profiles is a laborious and time-consuming process, especially when dealing with large volumes of constantly updated information on both candidates and jobs. Executive recruiters are required to spend significant hours manually evaluating matches. Without technological support, many recruitment firms may overlook ideal candidates, leading to missed hiring opportunities and reduced revenue.

This project aims to add an automated filtering layer using machine learning techniques, enabling recruiters to identify the top matches for candidates efficiently. With this model, recruiters can focus on the most relevant candidates and jobs, ultimately improving the match quality and reducing manual work. A successful model would decrease the number of hours spent on matching, improve match rates, and potentially support a wide range of additional recruitment use cases.

Furthermore, this project explores advanced NLP techniques, employing transformer-based encoding to process and interpret complex candidate and job data and as such has helped the author gain deeper understanding in NLP techniques. Through a comparative analysis of various models, this project demonstrates effective methods for evaluating and optimizing model performance for recruitment applications. 

# Project structure
```plaintext
├── inputs/                  # Raw and processed data files that would be inputted into the model
├── notebooks/               # Model wise Jupyter notebooks for exploratory analysis, preprocessing, featurization, and model training and evaluation
├── outputs/                 # Model wise processed data model for further use in developing an app and visualizations comparing the models
├── src/                     # Source code files
│   ├── getter/              # Scripts for fetching data
│   ├── preprocessing/       # Preprocessing scripts
│   ├── featurization/       # Feature engineering scripts
│   └── model training/      # Model training scripts
├── configuration            # Configuration file
├── requirements.txt         # Python dependencies
├── README.md                # Project overview (this file)
└── streamlit_app_all_models # Streamlit based application code for all models
```

# Data
### Data source
 

### Description
The original raw file contained as single large dataset comprising of 24 colums and just over 110000 rows. The data is a combination of both job information and the candidate inforamton. From the look at it, it looks like a data that was created by the recruiters or by someone who has evaluated the candidate at a certain level of interviewing stage. Each of the row implies a candidate associated witha particular job description. This means that we have a database of candidates applying for the particular role. Therefore establishing one to many relation between the job and candidate datasets. Having observed the data types for various columns, it is evident that the data has been obtained from the database collected through an application. 

The dataset consisted variety of datatypes for example: string, number, json objects. 

#### Job features
OpportunityId (class 'str') - Unique id for the job in the database
ExternalBriefDescription (class 'str') - Brief description of the job posting
ExternalDescription (class 'str') - Full description of the job posting
Title (class 'str') - Job posting's name
JobCategoryName (class 'str') - Predefined class to which the job posting belongs

#### Candidate features
- ApplicationId (class 'str') - Unique id for the applicant
- IsRejected (class 'numpy.bool') - Boolean value provided if the client was rejected in previous round
- IsCandidateInternal (class 'numpy.bool') - Boolean value refering if the candidate was refereed internally
- BehaviorCriteria (class 'numpy.ndarray') - Candidate's behaviour as per the criterion evaluated by the evaluator
- MotivationCriteria (class 'numpy.ndarray) - Candidate's motivation as per the criterion evaluated by the evaluator
- EducationCriteria (class 'numpy.ndarray') - Educational aspects of the candidate as per the criterion evaluated by an evaluator
- LicenseAndCertificationCriteria (class 'numpy.ndarray') - Licences and certifications aspects of the candidate as per the criterion evaluated by an evaluator
- SkillCriteria (class 'numpy.ndarray') - Skill sets as per the criterion evaluated by an evaluator
- WorkExperiences (class 'numpy.ndarray) - Details of the candidate's work experience
- Educations (class 'numpy.ndarray') - Details of the candidate's education
- LicenseAndCertifications (class 'numpy.ndarray') - Details associated with the candidate's licences and certifications
- Skills (class 'numpy.ndarray') - List of all relevant skills 
- Motivations (class 'numpy.ndarray') - Motivation criterion as listed by the candidate
- Behaviors (class 'numpy.ndarray) - Behavioural aspects listed by the candidate
- StepId (class 'str') - Unique id for the next step taken by the evaluator
- StepName (class 'str') - Details of the next step taken by the evaluator
- Tag (class 'numpy.float64') - Unknown number associated in the data base
- StepGroup (class 'str') - Category which the next step taken by the evaluator falls under
- pass_first_step (class 'numpy.bool') - Boolean informing if the data passed through the next step

### Data Preprocessing
- **Preprocessing**: The data used was preprocessed to cater to the input requirements of the various models viz. TF-IDF weighted word2vec, BERT based (uncased) and DistillBERT based (uncased) accordingly. In each of these cases, the first setp was to extract data from the array of objects to extract meaningful i.e. all data from key values inside each of the JSON objects was extracted and placed next to each other to form a string in separate columns. Standard None object was treated as None string. 

Further in word2vec based modelling in all of the string based columns, including the newly created columns mentioned above NLP steps like expanding contracted words, removal of stop words, Porter stemming were applied. Also, html and other programmatic tags were removed in this step. 

For transformer models, only the html and programmatic tags were removed as the transformer libraries are capable of handling texts without much preprocessing. 
- **Missing Data**: As such the data presented no missing values except in one column('Tag' class type 'int'). In this case all missing values were replaced with -1. 

### Featurization
- **Featurization**: Each of the preprocessed features is sent through a series of coded functions that encode text into vector embeddings for TFIDF Word2vector models. In case of transformer based models, standard transformer libraries by Hugging Face were used. For larger texts, the encodings were done sentence wise after tokenization using NLTK libraries and then a median vector was calculated

- **Approaches for Net Vector Calculation**: Two approaches were used to compute the net vectors: horizontal stacking and vertical stacking.

    - Horizontal Stacking: In this method, each feature (processed batch-wise if the feature contained a longer string) was encoded separately using each of the three NLP models. The resulting encoded vectors were placed side by side to create one long, high-dimensional vector. This approach extended the feature space, resulting in a single, comprehensive vector for each candidate or job.
    - Vertical Stacking: Here, the encoded vectors for each feature were stacked into an array. The final vector for each candidate or job was then computed by taking the median value across the stacked array. This approach produced shorter, uniformly sized vectors.

### Model Training
To compare cosine similarities between candidate and job vectors of different lengths generated by horizontal stacking, dimensionality reduction techniques like PCA were employed. Specifically, PCA was used to trim the higher-dimensional candidate vectors to match the dimensionality of the job vectors. In contrast, dimensionality reduction was not needed for vertical stacking, as the vectors were already of uniform length.


### Model Evaluation
The cosine similarities produced by each all combinations of job and candidate vectors are calculated and put under various statistical tests to compare the cosine-similarities across the models.

### Model Tuning - Feature Importance
The RMS value for a particular feature is determined by comparing the cosine similarities between the original data (with all features) and the data with one specific feature removed. A higher RMS value indicates a greater impact of the missing feature on the model's performance. Similarly, RMS values were calculated for each of the three models - for both vertical and horizontal stacking of the features.

# Features
All but StepId were used in modelling as it had no relevance being the unique id for the next step taken by the evaluator. The idea was also to evaluate which of these features could be removed later during model evaluation step. From a use-case standpoint, we could have removed a few unnecessary features before hand through the help of the subject experts but no such effort was made and this is being left as a future improvement case. Overall all the features were used. 


# Prerequisites
Python 3.6 or later preferably <br>
Jupyter Notebook <br>
Libraries listed in requirements.txt<br>

# Usage
For exploring the tasks, refer to the folders in  - notebooks/

To explore and run the various tasks, refer to the notebooks located in the notebooks/ directory. The folders within notebooks/ are organized in the order they should be executed, and their names are descriptive of their purpose. Below is a guide to each step:

#### 01_eda

**Purpose**: Conduct Exploratory Data Analysis (EDA) to gain initial insights into the data and understand its structure. Visualizations, graphs, and word clouds generated during this step are saved in the outputs folder. Run the below script to conduct the EDA. 

&emsp; &emsp; jupyter notebook notebooks/01_eda.ipynb

#### 02_preprocessing
**Purpose**: Perform data preprocessing steps as outlined in the Data section. This folder contains a single Jupyter notebook responsible for preparing the data for all subsequent models. To execute this step, run the following:

&emsp; &emsp; jupyter notebook notebooks/02_preprocessing.ipynb

#### 03_featurization
**Purpose**: Encode and transform the preprocessed data to create features suitable for model training. This step involves applying different featurization techniques based on the model requirements. To run the featurization for a particular model refer below code. 

- TFIDF-weighted Word2Vec Featurization:

&emsp; &emsp; &emsp; &emsp; jupyter notebook notebooks/3a_featurization.ipynb

 - BERT-based Uncased Model Featurization:

&emsp; &emsp; &emsp; &emsp; jupyter notebook notebooks/3b_featurization.ipynb

- DistilBERT-based Uncased Model Featurization:

&emsp; &emsp; &emsp; &emsp; jupyter notebook notebooks/3c_featurization.ipynb

#### 04_model_training
Purpose: Train models using the featurized data. This step includes dimensionality reduction and computation of cosine similarity between job and candidate data.

- Model Training with TFIDF-weighted Word2Vec Data:

&emsp; &emsp; &emsp; &emsp; jupyter notebook notebooks/4a_model_training.ipynb

- Model Training with BERT-based Uncased Data:

&emsp; &emsp; &emsp; &emsp; jupyter notebook notebooks/4b_model_training.ipynb

- Model Training with DistilBERT-based Uncased Data:

&emsp; &emsp; &emsp; &emsp; jupyter notebook notebooks/
4c_model_training.ipynb

#### 05_model_evaluation
Purpose: Evaluate the cosine similarity scores generated by each model and compare their performance. The results, including analysis and graphs, are saved in the outputs folder.

&emsp; &emsp;jupyter notebook notebooks/05_model_evaluation.ipynb

#### 06_model_tuning
Purpose: Fine-tune the models by testing the impact of removing individual features and assessing overall model performance. This step helps identify the best model configuration for further application development done.

&emsp; &emsp; jupyter notebook notebooks/06_model_tuning.ipynb

# Results
The results from this project show the comparative performance of the three NLP models—TF-IDF weighted Word2Vec, BERT-based (uncased), and DistilBERT-based (uncased) from featurization and cosine-similarity results model wise:

### Featurization:
#### Horizontal vs Vertical Stacking: 


Also, the RMS values become more pronounced in vertically stacked models, particularly when focusing on the top N number of vectors. This suggests that vertical stacking amplifies the differences among the top-performing features.

#### Model Performance: 
- **TF-IDF weighted Word2Vec**: This model showed relatively faster processing times, making it efficient even for larger datasets. In vertically stacked models, W2V demonstrates the best RMS values for the top N vectors, clearly outperforming BERT-based models in this configuration. Also, the RMS values for vertically stacked models are placed at much higher values compared to that of horizontally stacked vectors.
- **Transformer-based models**: <br>
*BERT-based (uncased)*: This model took longer processing times and  in comparison with TFIDF weighted Word2Vec model and did not demonstrate the expected superior ability to understand the context and subtle language features, resulting in only expending of computing resources for little or no benefit. <br>
*DistilBERT-based (uncased)*: This model provided a balance between speed and performance. While not as accurate as the full BERT model, it offered significantly faster processing times, however, being much higher than Word2Vec models.

#### Cosine Similarity Scores:
For each model, cosine similarity metrics were used to evaluate the strength of matches. The TFIDF weighted Word2Vector model consistently achieved the highest similarity scores, followed by BERT and then DistilBERT weighted Word2Vec.
Visualizations of similarity distributions shows that TFIDF weighted Word2Vectoris more consistent in clustering relevant job-candidate pairs closely together.

#### Feature Analysis:
The feature impact analysis indicated that there is no coherence of features between Word2Vec models and the transformer based models. However, the Word2Vec models produced more robust and intense RMS value when a feature is absent. As expected there was a coherence amongst the transformer models. 
But one aspect that cut across the transformer vs non-transformer models was that the features describing the job played an important role in the when non-transformer models were used. On contrary, the job features, like 'ExternalDescription' and 'Title,' did not contribute more significantly to matching accuracy. 
Overall, Removing non-essential features only marginally affected performanxce, suggesting potential for model optimization.

#### Comparison Insights:
The overall analysis revealed that Word2Vec model is less computationally expensive and is suitable for applciations demanding higher precision. 

# Limitations
Here are the key limitations so far identified with the project. By providing the limitations, we want to provide valuable insights: 
1. **Data Quality**: The project relies on a particular data obtained from the mentioned source. The data does contain some issues at both job and candidate features. In both these cases, when the unique ids are removed and checked for duplicates by only grouping the necessary features, both candidate and job features exhibhited high number of duplicates. This is probably owing to multiple entries made by the job seekers i.e. candidates and job providers at different points of time. However as the timestamp for each of the entries has not been provided in the original database. Therefore, each of the unique ids (i.e 'ApplicaitonId' and 'OpportunityId') are considered as final. if any additional information was provided, additional work could be done to eliminate duplicate candidature or jobs. The models nevertheless demonstrate and are constructed in right spirit, however if this information was provided, the models would need slight furhter modifications.  for demonstration purposes. Users are therefroe encourated to replace this data with their own to obtain the results. The quality or the results will depend on the input data provided.

2. **Scalability**: The code has been optimized for large datasets. When working with significantly larger datasets, users may encounter performnce bottlenects especailly when running large amount of texts through steps 3 onwards and therefore, scalling the project would require additional code optimization like GPU based processing and resource allocation. 

3. **No Guarantees of Futrue Development**: The extent of future development is dependant on the availability of the author's time. 

4. **Dependency on third party libraries**: The project evaluation is dependant on various libraries. Users should keep up to date and possibly modify the code as these libraries change and possibly notify authors for further updates. 

5. **Lack of official supoprt and official documentation**: This project is for demonstration purpose, so the documentation may be outdated or sometimes may lack all the information. In this case please feel free to send an email to rathishsekhar@gmail.com.  Also, there is no official supprot provided for this project. 

# Ethical Considerations
1. **No user identifiable data** is used in this project.
2. **Usage**: This project is for educational purposes both for the author and its users to understand various concepts surrounding the TF-IDF vectorization. Hence, no part of the code is to be used for any other purposes without adherance to ethical and legal guidelines of the general software development community. 

# Contributing
Contributions to this project are welcome. To contribute:

Fork the repository.
Create a new branch for your feature: git checkout -b feature-name
Make your changes and commit them: git commit -m 'Add feature-name'
Push to your branch: git push origin feature-name
Create a pull request.

# License
This project is licensed under the MIT License. Feel free to use and modify the code as needed.
# Acknowledgements
I would like to express my grattitude to Dr. Sima Sharifirad, who offered valuable feedback and insights throughout the project, overall playing a prominent role in the driving the direction of the project. Secondly, I thank the Hugging Face Inc. for creating sophisticated Transformer libraries and maintaining the resources. Further, the Streamlit community for their user-friendly framework that made the developement and deployment of the application efficient.

Lastly, I acknowledge the support of the open-source community and developer community for continual work to make complex libraries available to everyone. 

# Contact
For any questions or feedback, please contact:<br>
Name: Rathish Sekhar <br>
Email: rathishsekhar@gmail.com
