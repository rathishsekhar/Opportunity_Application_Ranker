{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b. Featurizing the data - BERT based uncased model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b.0 Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk; nltk.download(\"punkt\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm   \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b.1 Data\n",
    "### 3b.1.1 Gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OpportunityId</th>\n",
       "      <th>ApplicationId</th>\n",
       "      <th>ExternalBriefDescription</th>\n",
       "      <th>ExternalDescription</th>\n",
       "      <th>Title</th>\n",
       "      <th>JobCategoryName</th>\n",
       "      <th>IsRejected</th>\n",
       "      <th>IsCandidateInternal</th>\n",
       "      <th>BehaviorCriteria</th>\n",
       "      <th>MotivationCriteria</th>\n",
       "      <th>...</th>\n",
       "      <th>SkillCriteria__trnsfrmrpp</th>\n",
       "      <th>WorkExperiences__trnsfrmrpp</th>\n",
       "      <th>Educations__trnsfrmrpp</th>\n",
       "      <th>LicenseAndCertifications__trnsfrmrpp</th>\n",
       "      <th>Skills__trnsfrmrpp</th>\n",
       "      <th>Motivations__trnsfrmrpp</th>\n",
       "      <th>Behaviors__trnsfrmrpp</th>\n",
       "      <th>StepId__trnsfrmrpp</th>\n",
       "      <th>StepName__trnsfrmrpp</th>\n",
       "      <th>StepGroup__trnsfrmrpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MbzeABKVn06G8irkoHJeIg==</td>\n",
       "      <td>nTzdqGj020CYqTouPocGSg==</td>\n",
       "      <td>$16.00 Per Hour\\n\\nAt Orkin, our purpose is to...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;$16.00 Per Hour&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;&lt;s...</td>\n",
       "      <td>Customer Service Specialist</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'Description': 'Capable of carrying out a gi...</td>\n",
       "      <td>[{'Description': 'Inspired to perform well by ...</td>\n",
       "      <td>...</td>\n",
       "      <td>MinimumScaleValue 3 MinimumScaleValueName Inte...</td>\n",
       "      <td>EndMonth None EndYear None JobTitle Call Cente...</td>\n",
       "      <td>Degree Some college Description None Graduatio...</td>\n",
       "      <td></td>\n",
       "      <td>ScaleValue 4 ScaleValueName Advanced Skill Clo...</td>\n",
       "      <td>Description Inspired to perform well by moneta...</td>\n",
       "      <td>Description Devoted to a task or purpose with ...</td>\n",
       "      <td>K8yQlic+/UiXxBMpOnAoLQ==</td>\n",
       "      <td>Decline</td>\n",
       "      <td>declined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7SPt0A57/kyzM9hE9vxDRg==</td>\n",
       "      <td>QVk5MFCZ70WvlZE9FzAW9g==</td>\n",
       "      <td>$15.00 Per Hour\\n\\nAt Orkin, our purpose is to...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;$15.00 Per Hour&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;&lt;s...</td>\n",
       "      <td>Customer Service Specialist</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'Description': 'Capable of carrying out a gi...</td>\n",
       "      <td>[{'Description': 'Inspired to perform well by ...</td>\n",
       "      <td>...</td>\n",
       "      <td>MinimumScaleValue 3 MinimumScaleValueName Inte...</td>\n",
       "      <td>EndMonth None EndYear None JobTitle Coordinato...</td>\n",
       "      <td>Degree Diploma Description None GraduationMont...</td>\n",
       "      <td></td>\n",
       "      <td>ScaleValue 5 ScaleValueName Expert Skill Sales...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>K8yQlic+/UiXxBMpOnAoLQ==</td>\n",
       "      <td>Decline</td>\n",
       "      <td>declined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7SPt0A57/kyzM9hE9vxDRg==</td>\n",
       "      <td>I1kcPlAw3E+rqceh0qrutQ==</td>\n",
       "      <td>$15.00 Per Hour\\n\\nAt Orkin, our purpose is to...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;$15.00 Per Hour&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;&lt;s...</td>\n",
       "      <td>Customer Service Specialist</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'Description': 'Capable of carrying out a gi...</td>\n",
       "      <td>[{'Description': 'Inspired to perform well by ...</td>\n",
       "      <td>...</td>\n",
       "      <td>MinimumScaleValue 3 MinimumScaleValueName Inte...</td>\n",
       "      <td>EndMonth None EndYear None JobTitle Direct Car...</td>\n",
       "      <td>Degree HIGH SCHOOL DIPLOMA Description None Gr...</td>\n",
       "      <td></td>\n",
       "      <td>ScaleValue 4 ScaleValueName Advanced Skill Cas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>K8yQlic+/UiXxBMpOnAoLQ==</td>\n",
       "      <td>Decline</td>\n",
       "      <td>declined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zolSWBFjWESbfkj8AXLYwA==</td>\n",
       "      <td>VTCXZK6/ZUWJDpxTcm2CRg==</td>\n",
       "      <td>$15.00 Per Hour\\n\\nAt Orkin, our purpose is to...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;$15.00 Per Hour&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;&lt;s...</td>\n",
       "      <td>Customer Service Specialist</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'Description': 'Capable of carrying out a gi...</td>\n",
       "      <td>[{'Description': 'Inspired to perform well by ...</td>\n",
       "      <td>...</td>\n",
       "      <td>MinimumScaleValue 3 MinimumScaleValueName Inte...</td>\n",
       "      <td>EndMonth None EndYear 2019.0 JobTitle Package ...</td>\n",
       "      <td>Degree Associate in Early Description None Gra...</td>\n",
       "      <td></td>\n",
       "      <td>ScaleValue 5 ScaleValueName Expert Skill Cashi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>K8yQlic+/UiXxBMpOnAoLQ==</td>\n",
       "      <td>Decline</td>\n",
       "      <td>declined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zolSWBFjWESbfkj8AXLYwA==</td>\n",
       "      <td>I6KgcL0jdkG8wBnL1+BZ/g==</td>\n",
       "      <td>$15.00 Per Hour\\n\\nAt Orkin, our purpose is to...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;$15.00 Per Hour&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;&lt;s...</td>\n",
       "      <td>Customer Service Specialist</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'Description': 'Capable of carrying out a gi...</td>\n",
       "      <td>[{'Description': 'Inspired to perform well by ...</td>\n",
       "      <td>...</td>\n",
       "      <td>MinimumScaleValue 3 MinimumScaleValueName Inte...</td>\n",
       "      <td>EndMonth None EndYear None JobTitle Warehouse ...</td>\n",
       "      <td>Degree Bachelor of Business Admin Description ...</td>\n",
       "      <td></td>\n",
       "      <td>ScaleValue 5 ScaleValueName Expert Skill Forkl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>K8yQlic+/UiXxBMpOnAoLQ==</td>\n",
       "      <td>Decline</td>\n",
       "      <td>declined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              OpportunityId             ApplicationId  \\\n",
       "0  MbzeABKVn06G8irkoHJeIg==  nTzdqGj020CYqTouPocGSg==   \n",
       "1  7SPt0A57/kyzM9hE9vxDRg==  QVk5MFCZ70WvlZE9FzAW9g==   \n",
       "2  7SPt0A57/kyzM9hE9vxDRg==  I1kcPlAw3E+rqceh0qrutQ==   \n",
       "3  zolSWBFjWESbfkj8AXLYwA==  VTCXZK6/ZUWJDpxTcm2CRg==   \n",
       "4  zolSWBFjWESbfkj8AXLYwA==  I6KgcL0jdkG8wBnL1+BZ/g==   \n",
       "\n",
       "                            ExternalBriefDescription  \\\n",
       "0  $16.00 Per Hour\\n\\nAt Orkin, our purpose is to...   \n",
       "1  $15.00 Per Hour\\n\\nAt Orkin, our purpose is to...   \n",
       "2  $15.00 Per Hour\\n\\nAt Orkin, our purpose is to...   \n",
       "3  $15.00 Per Hour\\n\\nAt Orkin, our purpose is to...   \n",
       "4  $15.00 Per Hour\\n\\nAt Orkin, our purpose is to...   \n",
       "\n",
       "                                 ExternalDescription  \\\n",
       "0  <p><strong>$16.00 Per Hour</strong></p>\\n<p><s...   \n",
       "1  <p><strong>$15.00 Per Hour</strong></p>\\n<p><s...   \n",
       "2  <p><strong>$15.00 Per Hour</strong></p>\\n<p><s...   \n",
       "3  <p><strong>$15.00 Per Hour</strong></p>\\n<p><s...   \n",
       "4  <p><strong>$15.00 Per Hour</strong></p>\\n<p><s...   \n",
       "\n",
       "                         Title   JobCategoryName  IsRejected  \\\n",
       "0  Customer Service Specialist  Customer Service        True   \n",
       "1  Customer Service Specialist  Customer Service        True   \n",
       "2  Customer Service Specialist  Customer Service        True   \n",
       "3  Customer Service Specialist  Customer Service        True   \n",
       "4  Customer Service Specialist  Customer Service        True   \n",
       "\n",
       "   IsCandidateInternal                                   BehaviorCriteria  \\\n",
       "0                False  [{'Description': 'Capable of carrying out a gi...   \n",
       "1                False  [{'Description': 'Capable of carrying out a gi...   \n",
       "2                False  [{'Description': 'Capable of carrying out a gi...   \n",
       "3                False  [{'Description': 'Capable of carrying out a gi...   \n",
       "4                False  [{'Description': 'Capable of carrying out a gi...   \n",
       "\n",
       "                                  MotivationCriteria  ...  \\\n",
       "0  [{'Description': 'Inspired to perform well by ...  ...   \n",
       "1  [{'Description': 'Inspired to perform well by ...  ...   \n",
       "2  [{'Description': 'Inspired to perform well by ...  ...   \n",
       "3  [{'Description': 'Inspired to perform well by ...  ...   \n",
       "4  [{'Description': 'Inspired to perform well by ...  ...   \n",
       "\n",
       "                           SkillCriteria__trnsfrmrpp  \\\n",
       "0  MinimumScaleValue 3 MinimumScaleValueName Inte...   \n",
       "1  MinimumScaleValue 3 MinimumScaleValueName Inte...   \n",
       "2  MinimumScaleValue 3 MinimumScaleValueName Inte...   \n",
       "3  MinimumScaleValue 3 MinimumScaleValueName Inte...   \n",
       "4  MinimumScaleValue 3 MinimumScaleValueName Inte...   \n",
       "\n",
       "                         WorkExperiences__trnsfrmrpp  \\\n",
       "0  EndMonth None EndYear None JobTitle Call Cente...   \n",
       "1  EndMonth None EndYear None JobTitle Coordinato...   \n",
       "2  EndMonth None EndYear None JobTitle Direct Car...   \n",
       "3  EndMonth None EndYear 2019.0 JobTitle Package ...   \n",
       "4  EndMonth None EndYear None JobTitle Warehouse ...   \n",
       "\n",
       "                              Educations__trnsfrmrpp  \\\n",
       "0  Degree Some college Description None Graduatio...   \n",
       "1  Degree Diploma Description None GraduationMont...   \n",
       "2  Degree HIGH SCHOOL DIPLOMA Description None Gr...   \n",
       "3  Degree Associate in Early Description None Gra...   \n",
       "4  Degree Bachelor of Business Admin Description ...   \n",
       "\n",
       "  LicenseAndCertifications__trnsfrmrpp  \\\n",
       "0                                        \n",
       "1                                        \n",
       "2                                        \n",
       "3                                        \n",
       "4                                        \n",
       "\n",
       "                                  Skills__trnsfrmrpp  \\\n",
       "0  ScaleValue 4 ScaleValueName Advanced Skill Clo...   \n",
       "1  ScaleValue 5 ScaleValueName Expert Skill Sales...   \n",
       "2  ScaleValue 4 ScaleValueName Advanced Skill Cas...   \n",
       "3  ScaleValue 5 ScaleValueName Expert Skill Cashi...   \n",
       "4  ScaleValue 5 ScaleValueName Expert Skill Forkl...   \n",
       "\n",
       "                             Motivations__trnsfrmrpp  \\\n",
       "0  Description Inspired to perform well by moneta...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                               Behaviors__trnsfrmrpp  \\\n",
       "0  Description Devoted to a task or purpose with ...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "         StepId__trnsfrmrpp StepName__trnsfrmrpp StepGroup__trnsfrmrpp  \n",
       "0  K8yQlic+/UiXxBMpOnAoLQ==              Decline              declined  \n",
       "1  K8yQlic+/UiXxBMpOnAoLQ==              Decline              declined  \n",
       "2  K8yQlic+/UiXxBMpOnAoLQ==              Decline              declined  \n",
       "3  K8yQlic+/UiXxBMpOnAoLQ==              Decline              declined  \n",
       "4  K8yQlic+/UiXxBMpOnAoLQ==              Decline              declined  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moving to parent directory\n",
    "\n",
    "os.chdir(Path(os.path.realpath(\"\")).resolve().parents[1])\n",
    "\n",
    "# Importing the data gathering modules\n",
    "\n",
    "from src.getter.load_application_and_opportunity import *\n",
    "\n",
    "fdata = get_interim_data(\"preprocesseddata\")\n",
    "fdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OpportunityId', 'ApplicationId', 'ExternalBriefDescription',\n",
       "       'ExternalDescription', 'Title', 'JobCategoryName', 'IsRejected',\n",
       "       'IsCandidateInternal', 'BehaviorCriteria', 'MotivationCriteria',\n",
       "       'EducationCriteria', 'LicenseAndCertificationCriteria', 'SkillCriteria',\n",
       "       'WorkExperiences', 'Educations', 'LicenseAndCertifications', 'Skills',\n",
       "       'Motivations', 'Behaviors', 'StepId', 'StepName', 'Tag', 'StepGroup',\n",
       "       'pass_first_step', 'ExternalBriefDescription__pp',\n",
       "       'ExternalDescription__pp', 'Title__pp', 'JobCategoryName__pp',\n",
       "       'BehaviorCriteria__pp', 'MotivationCriteria__pp',\n",
       "       'EducationCriteria__pp', 'LicenseAndCertificationCriteria__pp',\n",
       "       'SkillCriteria__pp', 'WorkExperiences__pp', 'Educations__pp',\n",
       "       'LicenseAndCertifications__pp', 'Skills__pp', 'Motivations__pp',\n",
       "       'Behaviors__pp', 'StepId__pp', 'StepName__pp', 'StepGroup__pp',\n",
       "       'ExternalBriefDescription__w2vpp', 'ExternalDescription__w2vpp',\n",
       "       'Title__w2vpp', 'JobCategoryName__w2vpp', 'BehaviorCriteria__w2vpp',\n",
       "       'MotivationCriteria__w2vpp', 'EducationCriteria__w2vpp',\n",
       "       'LicenseAndCertificationCriteria__w2vpp', 'SkillCriteria__w2vpp',\n",
       "       'WorkExperiences__w2vpp', 'Educations__w2vpp',\n",
       "       'LicenseAndCertifications__w2vpp', 'Skills__w2vpp',\n",
       "       'Motivations__w2vpp', 'Behaviors__w2vpp', 'StepId__w2vpp',\n",
       "       'StepName__w2vpp', 'StepGroup__w2vpp',\n",
       "       'ExternalBriefDescription__trnsfrmrpp',\n",
       "       'ExternalDescription__trnsfrmrpp', 'Title__trnsfrmrpp',\n",
       "       'JobCategoryName__trnsfrmrpp', 'BehaviorCriteria__trnsfrmrpp',\n",
       "       'MotivationCriteria__trnsfrmrpp', 'EducationCriteria__trnsfrmrpp',\n",
       "       'LicenseAndCertificationCriteria__trnsfrmrpp',\n",
       "       'SkillCriteria__trnsfrmrpp', 'WorkExperiences__trnsfrmrpp',\n",
       "       'Educations__trnsfrmrpp', 'LicenseAndCertifications__trnsfrmrpp',\n",
       "       'Skills__trnsfrmrpp', 'Motivations__trnsfrmrpp',\n",
       "       'Behaviors__trnsfrmrpp', 'StepId__trnsfrmrpp', 'StepName__trnsfrmrpp',\n",
       "       'StepGroup__trnsfrmrpp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b.1.2 Defining column names for featurization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list containing names of the columns\n",
    "\n",
    "job_column = [\n",
    "    'ExternalBriefDescription',\n",
    "    'ExternalDescription', \n",
    "    'Title', \n",
    "    'JobCategoryName'\n",
    "]\n",
    "\n",
    "uid_column = ['OpportunityId', 'ApplicationId']\n",
    "\n",
    "# Column - 'Tag' Will be added later\n",
    "can_column = [\n",
    "    'IsCandidateInternal',\n",
    "    'BehaviorCriteria', \n",
    "    'MotivationCriteria',\n",
    "    'EducationCriteria', \n",
    "    'LicenseAndCertificationCriteria', \n",
    "    'SkillCriteria', \n",
    "    'WorkExperiences', \n",
    "    'Educations', \n",
    "    'LicenseAndCertifications', \n",
    "    'Skills', 'Motivations', \n",
    "    'Behaviors', \n",
    "    'StepName', \n",
    "    'Tag', \n",
    "    'StepGroup',\n",
    "    'pass_first_step'\n",
    "]\n",
    "\n",
    "sel_column = ['IsRejected']\n",
    "\n",
    "# Defining list of columns based on the type of contents\n",
    "\n",
    "str_column = [\n",
    "    'ExternalBriefDescription', \n",
    "    'ExternalDescription', \n",
    "    'Title', \n",
    "    'JobCategoryName', \n",
    "    'BehaviorCriteria', \n",
    "    'MotivationCriteria', \n",
    "    'EducationCriteria', \n",
    "    'LicenseAndCertificationCriteria', \n",
    "    'SkillCriteria', \n",
    "    'WorkExperiences', \n",
    "    'Educations', \n",
    "    'LicenseAndCertifications', \n",
    "    'Skills', \n",
    "    'Motivations', \n",
    "    'Behaviors', \n",
    "    'StepId', \n",
    "    'StepName', \n",
    "    'StepGroup'\n",
    "]\n",
    "\n",
    "bool_column = ['IsCandidateInternal', 'pass_first_step']\n",
    "\n",
    "float_column = ['Tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b.2 Generating embeddings using transformer based large language models\n",
    "### 3b.2.1 Observations on the data\n",
    "It was observed in our EDA that there were only 8473 opportunities, which implies the text in the string columns are likely to be repeated for same jobs. Also, even though the the applicaiton ID is unique, it is likely that these same candidate with details may have applied for multiple jobs. In order to reduce the time and space capacities, lets check if the string columns when put together contains any duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     101794\n",
      "False      8473\n",
      "Name: count, dtype: int64 candidate__str\n",
      "False    109754\n",
      "True        513\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110267"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gathering necessary columns of data \n",
    "\n",
    "fdata['opportunity__str'] = fdata[[m + \"__trnsfrmrpp\" \n",
    "                                   for m in job_column if m in str_column]\n",
    "                                   ].agg(\" \".join, axis = 1).tolist()\n",
    "\n",
    "fdata['candidate__str'] = fdata[[m + \"__trnsfrmrpp\" \n",
    "                                 for m in can_column if m in str_column]\n",
    "                                 ].agg( \" \".join, axis = 1).tolist()\n",
    "\n",
    "\n",
    "job_data = fdata[job_column + [\"OpportunityId\"]].duplicated()\n",
    "\n",
    " # ApplicationID ommitted as it was unique for all values\n",
    "\n",
    "can_data = fdata['candidate__str'].duplicated()\n",
    "\n",
    "print(job_data.value_counts(),  can_data.value_counts())\n",
    "fdata.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With so many (about 90%) duplicate values in candidate opportunities, it is prudent to therefore pass non-duplicates to the BERT and then associate the embeddings back to data.\n",
    "\n",
    "There wouldn't be significant time improvements(a reduction around 0.6% in the reduction of values) when pass non-duplicates to the BERT in case of the candidate textual information, yet we do it for the sake of uniformity in the code and execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b.2.2 Creating functions that derive bert based embeddings as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for function:\n",
    "\n",
    "def modelbased_embedder(data, uid_column_name, str_column, bool_column, float_column, hugging_face_model_name):\n",
    "    \"\"\"\n",
    "    Embeds textual data using a pre-trained transformer model from Hugging Face.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The input DataFrame containing textual data to be \n",
    "        embedded.\n",
    "        uid_column_name (str): The name of the column in the DataFrame that \n",
    "        contains unique identifiers for each row.\n",
    "        str_column (list): List containing the names(str) of columns onto which\n",
    "        modelbased_embedder are to be performed.\n",
    "        bool_column: List containing the names(str) of boolean columns\n",
    "        float_column: List containing the names(str) of the float data\n",
    "        hugging_face_model_name (str): The name or path of the pre-trained \n",
    "        transformer model from Hugging Face.\n",
    "\n",
    "    Returns:\n",
    "        dict_hstack (dict): A dictionary mapping unique identifiers to \n",
    "        horizontally stacked BERT embeddings.\n",
    "        dict_vstack (dict): A dictionary mapping unique identifiers to \n",
    "        vertically stacked mean-pooled BERT embeddings.\n",
    "\n",
    "    Notes:\n",
    "        1. This function pads the flaot and boolean data to the transformer's \n",
    "        vector size (i.e. config.hidden_size) so as to perform vector/tensor\n",
    "        mean(explained later). Therefore, in the horizontal \n",
    "        stacking/concatenating, the vector adds unnecessary dimensions. \n",
    "        2. This function uses the specified Hugging Face model to tokenize and \n",
    "        embed text data.\n",
    "        3. The embeddings are computed for each row in the input DataFrame, and \n",
    "        the results are stored in dictionaries.\n",
    "        4. Horizontal stacking (`job_opportunityid_bert_dict_hstack`) concatenates \n",
    "        embeddings for each column.\n",
    "        5. Vertical stacking (`job_opportunityid_bert_dict_vstack`) computes the \n",
    "        mean-pooled embeddings across all columns.\n",
    "        6. The resulting dictionaries can be used for downstream tasks such as \n",
    "        machine learning or similarity comparisons.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining functions that encode and pad boolean and float values\n",
    "\n",
    "    def encode_and_pad_boolean_columns(data, bool_column, vector_dim = 768):\n",
    "        \"\"\"\n",
    "        Encode bookean columns in a pandas DataFrame using OneHot Encoder\n",
    "        \n",
    "        Args:\n",
    "            data (pandas DataFrame): upon whose boolean columns the encoding is \n",
    "            to executed\n",
    "            bool_column (list): List containing the boolean columns names to be \n",
    "            encoded\n",
    "            vector_dim (int): Dimension of the w2v_vectors\n",
    "        \n",
    "        Returns:\n",
    "            None, modifies the DataFrame in place adding new columns with one \n",
    "            hot encoded data\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if 'OneHotEncoder' in dir():\n",
    "            onehotencoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
    "        else:\n",
    "            from sklearn.preprocessing import OneHotEncoder\n",
    "            onehotencoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
    "        \n",
    "        for colname in bool_column:\n",
    "            data[colname + \"__\"] = [\n",
    "                np.pad(x,  \n",
    "                    (0, vector_dim - (len(x) % vector_dim)), \n",
    "                    'constant') for x in onehotencoder.fit_transform(\n",
    "                        np.reshape(np.array(data[colname]), (-1, 1))\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "    def pad_float_columns(data, float_column, vector_dim = 768):\n",
    "        \"\"\"\n",
    "        Pads the specified float columns in the fdata pandas DataFrame so that \n",
    "        the final value has a length equal to vector_dim\n",
    "\n",
    "        Args:\n",
    "            fdata (pandas DataFrame): Data frame containing the float value\n",
    "            float_column (list): List of column names containing the float data\n",
    "            vector_dim (int): Dimension of the vector the columns will be \n",
    "            padded\n",
    "\n",
    "        Returns:\n",
    "            None: Converts/ modifies the data and generates the new columns\n",
    "        \"\"\"\n",
    "        for colname in float_column:\n",
    "            data[colname + \"__\"] = [np.pad(\n",
    "                x, \n",
    "                (0, vector_dim - (len(x) % vector_dim)), \n",
    "                'constant'\n",
    "            ) for x in (np.reshape(\n",
    "                np.array(data[colname]), (-1, 1)\n",
    "            ))]\n",
    "\n",
    "    # Main function begins  \n",
    "    # Applying hugging face model and tokenizer\n",
    "    # Loading model and tokenizer\n",
    "    '''Shifting to GPU for faster calculations - as applicable (user should \n",
    "    test if frequent transitioning between GPU and CPU, and viceversa \n",
    "    increases time-complexity)'''\n",
    "\n",
    "    # Use the below code on higher config machines\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    # elif torch.backends.mps.is_available():\n",
    "    #     device  = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\") \n",
    "    \n",
    "    if 'AutoTokenizer' in dir() and 'AutoModel' in dir():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hugging_face_model_name) \n",
    "        model = AutoModel.from_pretrained(hugging_face_model_name).to(device) \n",
    "\n",
    "    else:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hugging_face_model_name) \n",
    "        model = AutoModel.from_pretrained(hugging_face_model_name).to(device)   \n",
    "    \n",
    "    # Gathering data \n",
    "    data__ = data[\n",
    "        [uid_column_name]+ \n",
    "        [x + \"__trnsfrmrpp\" for x in str_column] + \n",
    "        bool_column + \n",
    "        float_column\n",
    "    ].drop_duplicates() \n",
    "\n",
    "    # Applying encode_pad_boolean_columns and pad_float_columns \n",
    "\n",
    "    encode_and_pad_boolean_columns(data__, bool_column, vector_dim = model.config.hidden_size)\n",
    "    pad_float_columns(data__, float_column, vector_dim = model.config.hidden_size)\n",
    "    \n",
    "    # Gathering and applying BERT base embedded vector for opportunity columns\n",
    "    \n",
    "    dict_hstack = {}\n",
    "    dict_vstack = {}\n",
    "    str_data__ = data__[[x + \"__trnsfrmrpp\" for x in str_column]] # Gathering string data only\n",
    "\n",
    "    for index, row in tqdm(\n",
    "        str_data__.iterrows(), desc = \"Processing rows\", total = len(data__)\n",
    "    ):\n",
    "        \n",
    "        embeddings_values = []\n",
    "        \n",
    "        for column in str_data__.columns:\n",
    "            text = str_data__.at[index, column]\n",
    "\n",
    "            # Taking care of empty text\n",
    "            if not text:\n",
    "                zero_embeddings = np.zeros((model.config.hidden_size,))\n",
    "                embeddings_values.append(zero_embeddings)\n",
    "                continue\n",
    "            \n",
    "            #Taking care of the blank [] obtained because of the lack of punctuation mark\n",
    "            if text and text[-1] not in \".?!\":\n",
    "                text+= \".\"\n",
    "\n",
    "        \n",
    "            sentences = nltk.tokenize.sent_tokenize(text)\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                sentences, \n",
    "                padding = True, \n",
    "                truncation = True, \n",
    "                return_tensors = \"pt\"\n",
    "            ).to(device) \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(**inputs)\n",
    "                \n",
    "            embeddings_values.append(\n",
    "                np.mean(\n",
    "                    output.last_hidden_state.mean(dim=1).cpu().numpy(), \n",
    "                    axis = 0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Now adding boolean and float values\n",
    "        # Adding padded bool_column values to the embeddings_values\n",
    "        for column in bool_column:\n",
    "            embeddings_values.append(data__.at[index, column + \"__\"])\n",
    "            \n",
    "        # Adding padded float_column values to the embeddings_values\n",
    "        for column in float_column:\n",
    "            embeddings_values.append(data__.at[index, column + \"__\"])\n",
    "\n",
    "        # Stacking the embeddings, boolean and float values \n",
    "        vector_h = np.hstack(tuple(embeddings_values))\n",
    "        vector_v = np.mean((tuple(embeddings_values)), axis = 0)\n",
    "        \n",
    "        dict_hstack[data__.at[index, uid_column_name]] = vector_h\n",
    "        dict_vstack[data__.at[index, uid_column_name]] = vector_v\n",
    "\n",
    "    return dict_hstack, dict_vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for function - Batch wise: - Created only for testing\n",
    "\n",
    "def modelbased_embedder_batched(data, uid_column_name, str_column, bool_column, float_column, hugging_face_model_name, batch_size = 32):\n",
    "    \"\"\"\n",
    "    Embeds textual data using a pre-trained transformer model from Hugging Face.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The input DataFrame containing textual data to be \n",
    "        embedded.\n",
    "        uid_column_name (str): The name of the column in the DataFrame that \n",
    "        contains unique identifiers for each row.\n",
    "        str_column (list): List containing the names(str) of columns onto which\n",
    "        modelbased_embedder are to be performed.\n",
    "        bool_column: List containing the names(str) of boolean columns\n",
    "        float_column: List containing the names(str) of the float data\n",
    "        hugging_face_model_name (str): The name or path of the pre-trained \n",
    "        transformer model from Hugging Face.\n",
    "\n",
    "    Returns:\n",
    "        dict_hstack (dict): A dictionary mapping unique identifiers to \n",
    "        horizontally stacked BERT embeddings.\n",
    "        dict_vstack (dict): A dictionary mapping unique identifiers to \n",
    "        vertically stacked mean-pooled BERT embeddings.\n",
    "\n",
    "    Notes:\n",
    "        1. This function pads the flaot and boolean data to the transformer's \n",
    "        vector size (i.e. config.hidden_size) so as to perform vector/tensor\n",
    "        mean(explained later). Therefore, in the horizontal \n",
    "        stacking/concatenating, the vector adds unnecessary dimensions. \n",
    "        2. This function uses the specified Hugging Face model to tokenize and \n",
    "        embed text data.\n",
    "        3. The embeddings are computed for each row in the input DataFrame, and \n",
    "        the results are stored in dictionaries.\n",
    "        4. Horizontal stacking (`job_opportunityid_bert_dict_hstack`) concatenates \n",
    "        embeddings for each column.\n",
    "        5. Vertical stacking (`job_opportunityid_bert_dict_vstack`) computes the \n",
    "        mean-pooled embeddings across all columns.\n",
    "        6. The resulting dictionaries can be used for downstream tasks such as \n",
    "        machine learning or similarity comparisons.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining functions that encode and pad boolean and float values\n",
    "\n",
    "    def encode_and_pad_boolean_columns(data, bool_column, vector_dim = 768):\n",
    "        \"\"\"\n",
    "        Encode bookean columns in a pandas DataFrame using OneHot Encoder\n",
    "        \n",
    "        Args:\n",
    "            data (pandas DataFrame): upon whose boolean columns the encoding is \n",
    "            to executed\n",
    "            bool_column (list): List containing the boolean columns names to be \n",
    "            encoded\n",
    "            vector_dim (int): Dimension of the w2v_vectors\n",
    "        \n",
    "        Returns:\n",
    "            None, modifies the DataFrame in place adding new columns with one \n",
    "            hot encoded data\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if 'OneHotEncoder' in dir():\n",
    "            onehotencoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
    "        else:\n",
    "            from sklearn.preprocessing import OneHotEncoder\n",
    "            onehotencoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
    "        \n",
    "        for colname in bool_column:\n",
    "            data[colname + \"__\"] = [\n",
    "                np.pad(x,  \n",
    "                    (0, vector_dim - (len(x) % vector_dim)), \n",
    "                    'constant') for x in onehotencoder.fit_transform(\n",
    "                        np.reshape(np.array(data[colname]), (-1, 1))\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "    def pad_float_columns(data, float_column, vector_dim = 768):\n",
    "        \"\"\"\n",
    "        Pads the specified float columns in the fdata pandas DataFrame so that \n",
    "        the final value has a length equal to vector_dim\n",
    "\n",
    "        Args:\n",
    "            fdata (pandas DataFrame): Data frame containing the float value\n",
    "            float_column (list): List of column names containing the float data\n",
    "            vector_dim (int): Dimension of the vector the columns will be \n",
    "            padded\n",
    "\n",
    "        Returns:\n",
    "            None: Converts/ modifies the data and generates the new columns\n",
    "        \"\"\"\n",
    "        for colname in float_column:\n",
    "            data[colname + \"__\"] = [np.pad(\n",
    "                x, \n",
    "                (0, vector_dim - (len(x) % vector_dim)), \n",
    "                'constant'\n",
    "            ) for x in (np.reshape(\n",
    "                np.array(data[colname]), (-1, 1)\n",
    "            ))]\n",
    "\n",
    "    # Main function begins  \n",
    "    # Applying hugging face model and tokenizer\n",
    "    # Loading model and tokenizer\n",
    "    '''Shifting to GPU for faster calculations - as applicable (user should \n",
    "    test if frequent transitioning between GPU and CPU, and viceversa \n",
    "    increases time-complexity)'''\n",
    "\n",
    "    # Use the below code on higher config machines\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    # elif torch.backends.mps.is_available():\n",
    "    #     device  = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\") \n",
    "    \n",
    "    if 'AutoTokenizer' in dir() and 'AutoModel' in dir():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hugging_face_model_name) \n",
    "        model = AutoModel.from_pretrained(hugging_face_model_name).to(device) \n",
    "\n",
    "    else:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hugging_face_model_name) \n",
    "        model = AutoModel.from_pretrained(hugging_face_model_name).to(device)   \n",
    "    \n",
    "    \n",
    "    # Gathering data \n",
    "    data__ = data[\n",
    "        [uid_column_name]+ \n",
    "        [x + \"__trnsfrmrpp\" for x in str_column] + \n",
    "        bool_column + \n",
    "        float_column\n",
    "    ].drop_duplicates()  \n",
    "\n",
    "    # Applying encode_pad_boolean_columns and pad_float_columns \n",
    "\n",
    "    encode_and_pad_boolean_columns(data__, bool_column, vector_dim = model.config.hidden_size)\n",
    "    pad_float_columns(data__, float_column, vector_dim = model.config.hidden_size)\n",
    "    \n",
    "    # Gathering and applying BERT base embedded vector for opportunity columns - batch wise\n",
    "\n",
    "    # Batch wise - \n",
    "\n",
    "    batches = [data__[i:i+batch_size] for i in range(0,len(data__), batch_size)]\n",
    "    \n",
    "    dict_hstack = {}\n",
    "    dict_vstack = {}\n",
    "\n",
    "    for batched_data in tqdm(batches, desc = \"Processing Batches\", total= math.ceil(len(data__)/batch_size)):\n",
    "        \n",
    "        str_data__ = batched_data[[x + \"__trnsfrmrpp\" for x in str_column]] # Gathering string data only\n",
    "\n",
    "        for index, row in str_data__.iterrows():\n",
    "            \n",
    "            embeddings_values = []\n",
    "            \n",
    "            for column in str_data__.columns:\n",
    "                text = str_data__.at[index, column]\n",
    "\n",
    "                # Taking care of empty text\n",
    "                if not text:\n",
    "                    zero_embeddings = np.zeros((model.config.hidden_size,))\n",
    "                    embeddings_values.append(zero_embeddings)\n",
    "                    continue\n",
    "                \n",
    "                #Taking care of the blank [] obtained because of the lack of punctuation mark\n",
    "                if text and text[-1] not in \".?!\":\n",
    "                    text+= \".\"\n",
    "\n",
    "            \n",
    "                sentences = nltk.tokenize.sent_tokenize(text)\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    sentences, \n",
    "                    padding = True, \n",
    "                    truncation = True, \n",
    "                    return_tensors = \"pt\"\n",
    "                ).to(device) \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model(**inputs)\n",
    "                    \n",
    "                embeddings_values.append(\n",
    "                    np.mean(\n",
    "                        output.last_hidden_state.mean(dim=1).cpu().numpy(), \n",
    "                        axis = 0\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            # Now adding boolean and float values\n",
    "            # Adding padded bool_column values to the embeddings_values\n",
    "            for column in bool_column:\n",
    "                embeddings_values.append(batched_data.at[index, column + \"__\"])\n",
    "                \n",
    "            # Adding padded float_column values to the embeddings_values\n",
    "            for column in float_column:\n",
    "                embeddings_values.append(batched_data.at[index, column + \"__\"])\n",
    "\n",
    "            # Stacking the embeddings, boolean and float values \n",
    "            vector_h = np.hstack(tuple(embeddings_values))\n",
    "            vector_v = np.mean((tuple(embeddings_values)), axis = 0)\n",
    "            \n",
    "            dict_hstack[data__.at[index, uid_column_name]] = vector_h\n",
    "            dict_vstack[data__.at[index, uid_column_name]] = vector_v\n",
    "\n",
    "    return dict_hstack, dict_vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b.2.3 Executing the modelbased_embedder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 8473/8473 [1:57:07<00:00,  1.21it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Gathering arguments for the modelbased_embedder function for Opportunity columns\n",
    "# data = fdata\n",
    "uid_column_name = 'OpportunityId'\n",
    "str_col = [x for x in job_column if x in str_column]\n",
    "bool_col = [x for x in job_column if x in bool_column]\n",
    "float_col = [x for x in job_column if x in float_column]\n",
    "hugging_face_model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Running the modelbased_embedder function\n",
    "job_opportunityid_bert_dict_hstack, job_opportunityid_bert_dict_vstack = modelbased_embedder(fdata, uid_column_name, str_col, bool_col, float_col, hugging_face_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 110267/110267 [20:42:38<00:00,  1.48it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Gathering arguments for the modelbased_embedder function for Candidate columns\n",
    "# data = fdata\n",
    "uid_column_name = 'ApplicationId'\n",
    "str_col = [x for x in can_column if x in str_column]\n",
    "bool_col = [x for x in can_column if x in bool_column]\n",
    "float_col = [x for x in can_column if x in float_column]\n",
    "hugging_face_model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Running the modelbased_embedder function\n",
    "can_applicationid_bert_dict_hstack, can_applicationid_bert_dict_vstack = modelbased_embedder(fdata, uid_column_name, str_col, bool_col, float_col, hugging_face_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288,) (3072,)\n",
      "(768,) (768,)\n"
     ]
    }
   ],
   "source": [
    "# Printing a random rows to see the shape of the dictionararies generated\n",
    "\n",
    "print(\n",
    "    can_applicationid_bert_dict_hstack['nTzdqGj020CYqTouPocGSg=='].shape, \n",
    "    job_opportunityid_bert_dict_hstack['MbzeABKVn06G8irkoHJeIg=='].shape\n",
    ")\n",
    "\n",
    "print(\n",
    "    can_applicationid_bert_dict_vstack['nTzdqGj020CYqTouPocGSg=='].shape, \n",
    "    job_opportunityid_bert_dict_vstack['MbzeABKVn06G8irkoHJeIg=='].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b.3 Saving data for futher analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules that save the data \n",
    "from src.getter.save_application_and_opportunity import save_interim_data\n",
    "\n",
    "# Adding dictionaries into the variables for pickle\n",
    "# Creating dictionary\n",
    "bert_dict = {}\n",
    "\n",
    "# Adding dictionaries\n",
    "bert_dict[\n",
    "    'job_opportunityid_bert_dict_hstack'\n",
    "    ] = job_opportunityid_bert_dict_hstack\n",
    "bert_dict[\n",
    "    'can_applicationid_bert_dict_hstack'\n",
    "    ] = can_applicationid_bert_dict_hstack\n",
    "\n",
    "bert_dict[\n",
    "    'job_opportunityid_bert_dict_vstack'\n",
    "    ] = job_opportunityid_bert_dict_vstack\n",
    "bert_dict[\n",
    "    'can_applicationid_bert_dict_vstack'\n",
    "    ] = can_applicationid_bert_dict_vstack\n",
    "\n",
    "# Saving variables dictionary\n",
    "\n",
    "save_interim_data(bert_dict, \"bert_data_dictionary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
