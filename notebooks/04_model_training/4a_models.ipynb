{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a. Models - Word2Vec model\n",
    "\n",
    "Description for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.0 Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.1 Gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing to project directory\n",
    "\n",
    "os.chdir(Path(os.path.realpath(\"\")).resolve().parents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules that fetch or save the data\n",
    "\n",
    "from src.getter.load_application_and_opportunity import *\n",
    "from src.getter.save_application_and_opportunity import *\n",
    "\n",
    "# Gathering the data\n",
    "w2v_data_dictionary = get_interim_data(\"w2v_data_dictionary\")\n",
    "ppdata = get_interim_data(\"preprocesseddata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list of column names that contains the names of the columns, if they belong to the job or candidate\n",
    "\n",
    "job_column = ['ExternalBriefDescription','ExternalDescription', 'Title', \n",
    "              'JobCategoryName']\n",
    "uid_column = ['OpportunityId', 'ApplicationId']\n",
    "can_column = [\n",
    "    'IsCandidateInternal',\n",
    "    'BehaviorCriteria', \n",
    "    'MotivationCriteria',\n",
    "    'EducationCriteria', \n",
    "    'LicenseAndCertificationCriteria', \n",
    "    'SkillCriteria', \n",
    "    'WorkExperiences', \n",
    "    'Educations', \n",
    "    'LicenseAndCertifications', \n",
    "    'Skills', \n",
    "    'Motivations', \n",
    "    'Behaviors', \n",
    "    'StepName', \n",
    "    'StepGroup',\n",
    "    'pass_first_step'\n",
    "] # Column - StepId has been removed\n",
    "\n",
    "sel_column = ['IsRejected']\n",
    "\n",
    "# Defining list of columns based on the type of contents\n",
    "\n",
    "str_column = [\n",
    "    'ExternalBriefDescription', \n",
    "    'ExternalDescription', \n",
    "    'Title', \n",
    "    'JobCategoryName', \n",
    "    'BehaviorCriteria', \n",
    "    'MotivationCriteria', \n",
    "    'EducationCriteria', \n",
    "    'LicenseAndCertificationCriteria', \n",
    "    'SkillCriteria', \n",
    "    'WorkExperiences', \n",
    "    'Educations', \n",
    "    'LicenseAndCertifications', \n",
    "    'Skills', \n",
    "    'Motivations', \n",
    "    'Behaviors', \n",
    "    'StepId', \n",
    "    'StepName', \n",
    "    'StepGroup'\n",
    "]\n",
    "bool_column = ['IsCandidateInternal', 'pass_first_step']\n",
    "float_column = ['Tag']\n",
    "\n",
    "# Defining list of columns based on the models\n",
    "\n",
    "model_names = [\"w2v\", \"bert\", \"dbert\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the data\n",
    "\n",
    "ppdata = ppdata[uid_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying w2v vectors onto the ppdata_uid \n",
    "\n",
    "ppdata[\"opportunity__w2v_hstack\"] = ppdata['OpportunityId'].apply(lambda x : w2v_data_dictionary['job_opportunityid_w2v_dict_hstack'][x])\n",
    "ppdata[\"opportunity__w2v_vstack\"] = ppdata['OpportunityId'].apply(lambda x : w2v_data_dictionary['job_opportunityid_w2v_dict_vstack'][x])\n",
    "ppdata[\"candidate__w2v_hstack\"] = ppdata['ApplicationId'].apply(lambda x : w2v_data_dictionary['can_applicationid_w2v_dict_hstack'][x])\n",
    "ppdata[\"candidate__w2v_vstack\"] = ppdata['ApplicationId'].apply(lambda x : w2v_data_dictionary['can_applicationid_w2v_dict_vstack'][x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.2.1 Working on word2vec data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through experimentation, it was realized that the word2vec or as a matter of fact any langugage based vector need not be scalled as scalling would modify the information stored in the embeddings , we do not scale the data before PCA.  \n",
    "\n",
    "#### 4a.2.1.1 Creating functions that reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating function that creates arrays for calculating the cosine similarity \n",
    "def reduce_dimensionality(data, opp_uid_name, app_uid_name,  opportunity_stack_dict, candidate_stack_dict):\n",
    "     \"\"\"\n",
    "    Reduce the dimensionality of vectors using PCA, if horizontally stacked \n",
    "    (ignored if vertically stacked) and create arrays from the data used \n",
    "    further for similarity calculations\n",
    "    \n",
    "    Parameters:\n",
    "        candidate_vectors (pandas.DataFrame): Horizontally stacked Word2Vec \n",
    "        vectors for candidates\n",
    "        opportunity_vectors (pandas.DataFrame): Horizontally stacked Word2Vec \n",
    "        vectors for opportunities\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Transformed array of vectors with reduced dimensionality\n",
    "        if horizontally stacked else transformed array of vectors with same \n",
    "        dimensionality\n",
    "    \"\"\"\n",
    "     data__ = data[[opp_uid_name] + [app_uid_name]]\n",
    "     data__['opportunity_vectors'] = data__[opp_uid_name].apply(lambda x : opportunity_stack_dict[x])\n",
    "     data__['candidate_vectors'] = data__[app_uid_name].apply(lambda x : candidate_stack_dict[x])\n",
    "\n",
    "     opportunity__ = np.array(\n",
    "          [np.array(x) for x in data__['opportunity_vectors'].tolist()]\n",
    "        )\n",
    "    \n",
    "     candidate__ = np.array(\n",
    "          [np.array(x) for x in data__['candidate_vectors'].tolist()]\n",
    "        )\n",
    "\n",
    "    # Setting the number of dimensions as minimum shape\n",
    "     no_of_dimensions = min(candidate__.shape[1], opportunity__.shape[1])\n",
    "\n",
    "     if 'PCA' in dir():\n",
    "        pca = PCA(n_components = no_of_dimensions)\n",
    "     else:\n",
    "         from sklearn.decomposition import PCA \n",
    "         pca =  PCA(n_components = no_of_dimensions)\n",
    "\n",
    "     # Applying PCA \n",
    "     if candidate__.shape[1] >= opportunity__.shape[1]: \n",
    "          # Exporting pca.fit for app based requirement\n",
    "         pca_fit = pca.fit(candidate__)\n",
    "         save_app_data(pca_fit, 'candidate_w2v_pca_model')\n",
    "\n",
    "         app_array, opp_array = pca.fit_transform(candidate__), opportunity__\n",
    "     else:\n",
    "          # Exporting pca.fit for app based requirement\n",
    "         pca_fit = pca.fit(opportunity__)\n",
    "         save_app_data(pca_fit, 'opportunity_w2v_pca_model')\n",
    "\n",
    "         app_array, opp_array = candidate__, pca.fit_transform(opportunity__) \n",
    "    \n",
    "     app_pca_dict, opp_pca_dict = {}, {}\n",
    "     \n",
    "     for uid, vector in zip(data__[app_uid_name], app_array):\n",
    "         app_pca_dict[uid] = vector\n",
    "     \n",
    "     for uid, vector in zip(data__[opp_uid_name], opp_array):\n",
    "         opp_pca_dict[uid] = vector\n",
    "    \n",
    "     return opp_pca_dict, app_pca_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a.2.1.2 Deriving dimensionally reduced dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Deriving dimensionally reduced dictionaries for opportunity ID\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m opp__w2v_pca_hstack_dict, app__w2v_pca_hstack_dict \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_dimensionality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpportunityId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mApplicationId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2v_data_dictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjob_opportunityid_w2v_dict_hstack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2v_data_dictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcan_applicationid_w2v_dict_hstack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m opp__w2v_pca_vstack_dict, app__w2v_pca_vstack_dict \u001b[38;5;241m=\u001b[39m reduce_dimensionality(ppdata, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpportunityId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplicationId\u001b[39m\u001b[38;5;124m\"\u001b[39m, w2v_data_dictionary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_opportunityid_w2v_dict_vstack\u001b[39m\u001b[38;5;124m'\u001b[39m], w2v_data_dictionary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan_applicationid_w2v_dict_vstack\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m, in \u001b[0;36mreduce_dimensionality\u001b[0;34m(data, opp_uid_name, app_uid_name, opportunity_stack_dict, candidate_stack_dict)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Applying PCA \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m candidate__\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m opportunity__\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]: \n\u001b[1;32m     42\u001b[0m      \u001b[38;5;66;03m# Exporting pca.fit for app based requirement\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     pca_fit \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate__\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     save_app_data(pca_fit, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandidate_w2v_pca_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m     app_array, opp_array \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(candidate__), opportunity__\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:428\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:516\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_full(X, n_components)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_truncated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_svd_solver\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:656\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[0;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[1;32m    652\u001b[0m     U, Vt \u001b[38;5;241m=\u001b[39m svd_flip(U[:, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], Vt[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# sign flipping is done inside\u001b[39;00m\n\u001b[0;32m--> 656\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_svd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_oversamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_oversamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterated_power\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflip_sign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_ \u001b[38;5;241m=\u001b[39m n_samples\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m Vt\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/extmath.py:494\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 494\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_range_finder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[1;32m    503\u001b[0m B \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m M\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/extmath.py:311\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Perform power iterations with Q to further 'imprint' the top\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# singular vectors of A in Q\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m--> 311\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m normalizer(A\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m Q)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Sample the range of A using by linear projection of Q\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Extract an orthonormal basis\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/scipy/linalg/_decomp_lu.py:159\u001b[0m, in \u001b[0;36mlu\u001b[0;34m(a, permute_l, overwrite_a, check_finite, p_indices)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124millegal value in \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mth argument of internal gesv|posv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    156\u001b[0m                      \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m-\u001b[39minfo)\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlu\u001b[39m(a, permute_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    160\u001b[0m        p_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Compute LU decomposition of a matrix with partial pivoting.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite(a) \u001b[38;5;28;01mif\u001b[39;00m check_finite \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Deriving dimensionally reduced dictionaries for opportunity ID\n",
    "opp__w2v_pca_hstack_dict, app__w2v_pca_hstack_dict = reduce_dimensionality(ppdata, \"OpportunityId\", \"ApplicationId\", w2v_data_dictionary['job_opportunityid_w2v_dict_hstack'], w2v_data_dictionary['can_applicationid_w2v_dict_hstack'])\n",
    "opp__w2v_pca_vstack_dict, app__w2v_pca_vstack_dict = reduce_dimensionality(ppdata, \"OpportunityId\", \"ApplicationId\", w2v_data_dictionary['job_opportunityid_w2v_dict_vstack'], w2v_data_dictionary['can_applicationid_w2v_dict_vstack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimensions after PCA\n",
    "\n",
    "print(len(opp__w2v_pca_hstack_dict), len(app__w2v_pca_hstack_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a.2.2.1 Creating functions that calculate cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top n-similar cosine vectors\n",
    "def pairwise_cosine(data, opp_uid_name, app_uid_name, opp_pca_dict, app_pca_dict):\n",
    "    '''\n",
    "    Compute pairwise cosine similarity between opportunity and application \n",
    "    vectors in DataFrame\n",
    "\n",
    "    Parameters:\n",
    "        data(pandas.DataFrame): Input DataFrame containg opportunity and \n",
    "        application UIDs. \n",
    "        opp_upd_name(str): Columns name with oppotunity IDs\n",
    "        app_uid_name(str): Columns name with applciation IDs\n",
    "        opp_pca_dict (dict): Dictionary mapping opportunity uids to the vectors\n",
    "        app_pca_dict (dict): Dictionary mapping application uids to the vectors\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame with columns \"OpportunityId\", \"ApplciationId and \n",
    "        cosine similarity\n",
    "    '''\n",
    "    data__ = data[[opp_uid_name] + [app_uid_name]]\n",
    "    data__['opportunity_vectors'] = data__[opp_uid_name].apply(lambda x : opp_pca_dict[x])\n",
    "    data__['candidate_vectors'] = data__[app_uid_name].apply(lambda x : app_pca_dict[x])\n",
    "\n",
    "    opportunity__ = np.array(\n",
    "          [np.array(x) for x in data__['opportunity_vectors'].tolist()]\n",
    "        )\n",
    "    \n",
    "    candidate__ = np.array(\n",
    "          [np.array(x) for x in data__['candidate_vectors'].tolist()]\n",
    "        )\n",
    "    \n",
    "    opportunity__ = (opportunity__/np.linalg.norm(opportunity__, axis = 1)[:, np.newaxis])\n",
    "    candidate__ = (candidate__/np.linalg.norm(candidate__, axis = 1)[:, np.newaxis])\n",
    "    \n",
    "    data__['row_similarity'] = [np.dot(row1, row2) for row1, row2 in zip(opportunity__, candidate__)]\n",
    "     \n",
    "    return data__[[opp_uid_name] + [app_uid_name] + ['row_similarity']]\n",
    "\n",
    "def topn_similar(opp_pca_dict, app_pca_dict, n = 3):\n",
    "    \"\"\"\n",
    "    Calculates top n most similar application IDs for a given opportunity ID\n",
    "\n",
    "    Parameters:\n",
    "        opp_pca_dict (dict): Dictionary mapping opportunity IDs to their \n",
    "        dimensionally reduced vectors. \n",
    "        app_pca_dict (dict): Dictionary mapping application IDs to their \n",
    "        dimensionally reduced vectors. \n",
    "        n(int, optional - default = 3): Number of top similar application IDs \n",
    "        to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with keys as opportunity IDs pointing to value which is \n",
    "    a dictionary in itself. This dictionary contaings application IDs as keys \n",
    "    and similarity scores as values\n",
    "    \"\"\"\n",
    "    \n",
    "    similarity_dict = {}\n",
    "\n",
    "    for key_opp, val_opp in tqdm(opp_pca_dict.items(), desc = \"Application IDs: \", total = len(opp_pca_dict)):\n",
    "        \n",
    "        temp = {}\n",
    "\n",
    "        for key_app, val_app in app_pca_dict.items():\n",
    "            temp_value = np.dot(val_opp/np.linalg.norm(val_opp), val_app/np.linalg.norm(val_app))\n",
    "            temp[key_app] = temp_value\n",
    "        \n",
    "        sorteddict = sorted(temp.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "        similarity_dict[key_opp] = sorteddict\n",
    "    \n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a.2.2.2 Applying pariwise - cosine similarity and getting top n(=3 default) similar application IDs and similarity values for each opportunity ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine-similarity pairwise\n",
    "cosine_similarity_w2v_opp_app_hstack = pairwise_cosine(ppdata, \"OpportunityId\", \"ApplicationId\", opp__w2v_pca_hstack_dict, app__w2v_pca_hstack_dict)\n",
    "cosine_similarity_w2v_opp_app_vstack = pairwise_cosine(ppdata, \"OpportunityId\", \"ApplicationId\", opp__w2v_pca_vstack_dict, app__w2v_pca_vstack_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity dictionaries Space\n",
    "similarity_w2v_dict_opp_app_hstack = topn_similar(opp__w2v_pca_hstack_dict, app__w2v_pca_hstack_dict)\n",
    "similarity_w2v_dict_opp_app_vstack = topn_similar(opp__w2v_pca_vstack_dict, app__w2v_pca_vstack_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.2.3 Saving the similarity data - horizontally and vertically stacked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data \n",
    "# hstack\n",
    "save_processed_data(similarity_w2v_dict_opp_app_hstack, \"similarity_dict_w2v_hstack\")\n",
    "save_processed_data(cosine_similarity_w2v_opp_app_hstack, \"cosine_similarity_w2v_hstack\")\n",
    "\n",
    "#vstack\n",
    "save_processed_data(similarity_w2v_dict_opp_app_vstack, \"similarity_dict_w2v_vstack\")\n",
    "save_processed_data(cosine_similarity_w2v_opp_app_vstack, \"cosine_similarity_w2v_vstack\")\n",
    "\n",
    "#Saving the dimensionally reduced vectors for streamlit app output\n",
    "save_app_data(opp__w2v_pca_hstack_dict, 'opp__w2v_pca_hstack_dict')\n",
    "save_app_data(app__w2v_pca_hstack_dict, 'app__w2v_pca_hstack_dict')\n",
    "save_app_data(opp__w2v_pca_vstack_dict, 'opp__w2v_pca_vstack_dict')\n",
    "save_app_data(app__w2v_pca_vstack_dict, 'app__w2v_pca_vstack_dict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
