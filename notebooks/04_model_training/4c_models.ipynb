{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c. Models - distill BERT\n",
    "\n",
    "Description of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c.0 Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c.1 Gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing to project directory\n",
    "\n",
    "os.chdir(Path(os.path.realpath(\"\")).resolve().parents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules that fetch or save data\n",
    "\n",
    "from src.getter.load_application_and_opportunity import *\n",
    "from src.getter.save_application_and_opportunity import *\n",
    "\n",
    "# Gathering the data\n",
    "\n",
    "dbert_data_dictionary = get_interim_data(\"dbert_data_dictionary\")\n",
    "ppdata = get_interim_data(\"preprocesseddata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list of column names that contains the names of the columns, if they belong to the job or candidate\n",
    "\n",
    "job_column = ['ExternalBriefDescription','ExternalDescription', 'Title', \n",
    "              'JobCategoryName']\n",
    "uid_column = ['OpportunityId', 'ApplicationId']\n",
    "can_column = [\n",
    "    'IsCandidateInternal',\n",
    "    'BehaviorCriteria', \n",
    "    'MotivationCriteria',\n",
    "    'EducationCriteria', \n",
    "    'LicenseAndCertificationCriteria', \n",
    "    'SkillCriteria', \n",
    "    'WorkExperiences', \n",
    "    'Educations', \n",
    "    'LicenseAndCertifications', \n",
    "    'Skills', \n",
    "    'Motivations', \n",
    "    'Behaviors', \n",
    "    'StepName', \n",
    "    'StepGroup',\n",
    "    'pass_first_step'\n",
    "] # Column - StepId has been removed\n",
    "sel_column = ['IsRejected']\n",
    "\n",
    "# Defining list of columns based on the type of contents\n",
    "\n",
    "str_column = [\n",
    "    'ExternalBriefDescription', \n",
    "    'ExternalDescription', \n",
    "    'Title', \n",
    "    'JobCategoryName', \n",
    "    'BehaviorCriteria', \n",
    "    'MotivationCriteria', \n",
    "    'EducationCriteria', \n",
    "    'LicenseAndCertificationCriteria', \n",
    "    'SkillCriteria', \n",
    "    'WorkExperiences', \n",
    "    'Educations', \n",
    "    'LicenseAndCertifications', \n",
    "    'Skills', \n",
    "    'Motivations', \n",
    "    'Behaviors', \n",
    "    'StepId', \n",
    "    'StepName', \n",
    "    'StepGroup'\n",
    "]\n",
    "bool_column = ['IsCandidateInternal', 'pass_first_step']\n",
    "float_column = ['Tag']\n",
    "\n",
    "# Defining list of columns based on the models\n",
    "\n",
    "model_names = [\"w2v\", \"bert\", \"dbert\"]\n",
    "\n",
    "# Setting the local folder location\n",
    "dataloc = '/Users/rathish/Documents/Projects/Opportunity_Application_Ranker/inputs/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the data\n",
    "ppdata = ppdata[uid_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying distil-bert vectors onto the ppdata_uid \n",
    "\n",
    "ppdata[\"opportunity__dbert_hstack\"] = ppdata['OpportunityId'].apply(lambda x : dbert_data_dictionary['job_opportunityid_dbert_dict_hstack'][x])\n",
    "ppdata[\"opportunity__dbert_vstack\"] = ppdata['OpportunityId'].apply(lambda x : dbert_data_dictionary['job_opportunityid_dbert_dict_vstack'][x])\n",
    "ppdata[\"candidate__dbert_hstack\"] = ppdata['ApplicationId'].apply(lambda x : dbert_data_dictionary['can_applicationid_dbert_dict_hstack'][x])\n",
    "ppdata[\"candidate__dbert_vstack\"] = ppdata['ApplicationId'].apply(lambda x : dbert_data_dictionary['can_applicationid_dbert_dict_vstack'][x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c.2.1 Working on distill-BERT data\n",
    "\n",
    "Through experimentation, it was realized that the dbert or as a matter of fact any langugage based vector need not be scalled as scalling would modify the information stored in the embeddings , we do not scale the data before PCA.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating function that creates arrays for calculating the cosine similarity \n",
    "def reduce_dimensionality(data, opp_uid_name, app_uid_name,  opportunity_stack_dict, candidate_stack_dict):\n",
    "     \"\"\"\n",
    "    Reduce the dimensionality of vectors using PCA, if horizontally stacked \n",
    "    (ignored if vertically stacked) and create arrays from the data used \n",
    "    further for similarity calculations\n",
    "    \n",
    "    Parameters:\n",
    "        candidate_vectors (pandas.DataFrame): Horizontally stacked Word2Vec \n",
    "        vectors for candidates\n",
    "        opportunity_vectors (pandas.DataFrame): Horizontally stacked Word2Vec \n",
    "        vectors for opportunities\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Transformed array of vectors with reduced dimensionality\n",
    "        if horizontally stacked else transformed array of vectors with same \n",
    "        dimensionality\n",
    "    \"\"\"\n",
    "     data__ = data[[opp_uid_name] + [app_uid_name]]\n",
    "     data__['opportunity_vectors'] = data__[opp_uid_name].apply(lambda x : opportunity_stack_dict[x])\n",
    "     data__['candidate_vectors'] = data__[app_uid_name].apply(lambda x : candidate_stack_dict[x])\n",
    "\n",
    "     opportunity__ = np.array(\n",
    "          [np.array(x) for x in data__['opportunity_vectors'].tolist()]\n",
    "        )\n",
    "    \n",
    "     candidate__ = np.array(\n",
    "          [np.array(x) for x in data__['candidate_vectors'].tolist()]\n",
    "        )\n",
    "\n",
    "    # Setting the number of dimensions as minimum shape\n",
    "     no_of_dimensions = min(candidate__.shape[1], opportunity__.shape[1])\n",
    "\n",
    "     if 'PCA' in dir():\n",
    "        pca = PCA(n_components = no_of_dimensions)\n",
    "     else:\n",
    "         from sklearn.decomposition import PCA \n",
    "         pca =  PCA(n_components = no_of_dimensions)\n",
    "\n",
    "     # Applying PCA \n",
    "     if candidate__.shape[1] >= opportunity__.shape[1]:\n",
    "         # Exporting pca.fit for app based requirement\n",
    "         pca_fit = pca.fit(candidate__)\n",
    "         save_app_data(pca_fit, 'candidate_dbert_pca_model')\n",
    "\n",
    "         app_array, opp_array = pca.fit_transform(candidate__), opportunity__\n",
    "     else:\n",
    "         # Exporting pca.fit for app based requirement\n",
    "         pca_fit = pca.fit(opportunity__)\n",
    "         save_app_data(pca_fit, 'opportunity_dbert_pca_model')\n",
    "\n",
    "         app_array, opp_array = candidate__, pca.fit_transform(opportunity__) \n",
    "    \n",
    "     app_pca_dict, opp_pca_dict = {}, {}\n",
    "     \n",
    "     for uid, vector in zip(data__[app_uid_name], app_array):\n",
    "         app_pca_dict[uid] = vector\n",
    "     \n",
    "     for uid, vector in zip(data__[opp_uid_name], opp_array):\n",
    "         opp_pca_dict[uid] = vector\n",
    "    \n",
    "     return opp_pca_dict, app_pca_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c.2.1.2 Deriving dimensionally reduced dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the necessary data for future analysis\n",
    "# Deriving dimensionally reduced dictionaries for opportunity ID\n",
    "opp__dbert_pca_hstack_dict, app__dbert_pca_hstack_dict = reduce_dimensionality(ppdata, \"OpportunityId\", \"ApplicationId\", dbert_data_dictionary['job_opportunityid_dbert_dict_hstack'], dbert_data_dictionary['can_applicationid_dbert_dict_hstack'])\n",
    "\n",
    "\n",
    "# Beware of using this, this functions overwrites the PCA pickle file - Use with caution\n",
    "# opp__dbert_pca_vstack_dict, app__dbert_pca_vstack_dict = reduce_dimensionality(ppdata, \"OpportunityId\", \"ApplicationId\", dbert_data_dictionary['job_opportunityid_dbert_dict_vstack'], dbert_data_dictionary['can_applicationid_dbert_dict_vstack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8473 110267\n"
     ]
    }
   ],
   "source": [
    "# Checking the dimension after PCA\n",
    "print(len(opp__dbert_pca_hstack_dict), len(app__dbert_pca_hstack_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top n-similar cosine vectors\n",
    "def pairwise_cosine(data, opp_uid_name, app_uid_name, opp_pca_dict, app_pca_dict):\n",
    "    '''\n",
    "    Compute pairwise cosine similarity between opportunity and application \n",
    "    vectors in DataFrame\n",
    "\n",
    "    Parameters:\n",
    "        data(pandas.DataFrame): Input DataFrame containg opportunity and \n",
    "        application UIDs. \n",
    "        opp_upd_name(str): Columns name with oppotunity IDs\n",
    "        app_uid_name(str): Columns name with applciation IDs\n",
    "        opp_pca_dict (dict): Dictionary mapping opportunity uids to the vectors\n",
    "        app_pca_dict (dict): Dictionary mapping application uids to the vectors\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame with columns \"OpportunityId\", \"ApplciationId and \n",
    "        cosine similarity\n",
    "    '''\n",
    "    data__ = data[[opp_uid_name] + [app_uid_name]]\n",
    "    data__['opportunity_vectors'] = data__[opp_uid_name].apply(lambda x : opp_pca_dict[x])\n",
    "    data__['candidate_vectors'] = data__[app_uid_name].apply(lambda x : app_pca_dict[x])\n",
    "\n",
    "    opportunity__ = np.array(\n",
    "          [np.array(x) for x in data__['opportunity_vectors'].tolist()]\n",
    "        )\n",
    "    \n",
    "    candidate__ = np.array(\n",
    "          [np.array(x) for x in data__['candidate_vectors'].tolist()]\n",
    "        )\n",
    "    \n",
    "    opportunity__ = (opportunity__/np.linalg.norm(opportunity__, axis = 1)[:, np.newaxis])\n",
    "    candidate__ = (candidate__/np.linalg.norm(candidate__, axis = 1)[:, np.newaxis])\n",
    "    \n",
    "    data__['row_similarity'] = [np.dot(row1, row2) for row1, row2 in zip(opportunity__, candidate__)]\n",
    "     \n",
    "    return data__[[opp_uid_name] + [app_uid_name] + ['row_similarity']]\n",
    "\n",
    "def topn_similar(opp_pca_dict, app_pca_dict, n = 3):\n",
    "    \"\"\"\n",
    "    Calculates top n most similar application IDs for a given opportunity ID\n",
    "\n",
    "    Parameters:\n",
    "        opp_pca_dict (dict): Dictionary mapping opportunity IDs to their \n",
    "        dimensionally reduced vectors. \n",
    "        app_pca_dict (dict): Dictionary mapping application IDs to their \n",
    "        dimensionally reduced vectors. \n",
    "        n(int, optional - default = 3): Number of top similar application IDs \n",
    "        to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with keys as opportunity IDs pointing to value which is \n",
    "    a dictionary in itself. This dictionary contaings application IDs as keys \n",
    "    and similarity scores as values\n",
    "    \"\"\"\n",
    "    \n",
    "    similarity_dict = {}\n",
    "\n",
    "    for key_opp, val_opp in tqdm(opp_pca_dict.items(), desc = \"Application IDs: \", total = len(opp_pca_dict)):\n",
    "        \n",
    "        temp = {}\n",
    "\n",
    "        for key_app, val_app in app_pca_dict.items():\n",
    "            temp_value = np.dot(val_opp/np.linalg.norm(val_opp), val_app/np.linalg.norm(val_app))\n",
    "            temp[key_app] = temp_value\n",
    "        \n",
    "        sorteddict = sorted(temp.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "        similarity_dict[key_opp] = sorteddict\n",
    "    \n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c.2.2.2 Applying pariwise - cosine similarity and getting top n(=3 default) similar application IDs and similarity values for each opportunity ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine-similarities pairwise\n",
    "cosine_similarity_dbert_opp_app_hstack = pairwise_cosine(ppdata, \"OpportunityId\", \"ApplicationId\", opp__dbert_pca_hstack_dict, app__dbert_pca_hstack_dict)\n",
    "cosine_similarity_dbert_opp_app_vstack = pairwise_cosine(ppdata, \"OpportunityId\", \"ApplicationId\", opp__dbert_pca_vstack_dict, app__dbert_pca_vstack_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Application IDs: 100%|██████████| 8473/8473 [15:28:28<00:00,  6.57s/it]      \n",
      "Application IDs: 100%|██████████| 8473/8473 [8:06:39<00:00,  3.45s/it]     \n"
     ]
    }
   ],
   "source": [
    "# Similarity dictionaries\n",
    "similarity_dbert_dict_opp_app_hstack = topn_similar(opp__dbert_pca_hstack_dict, app__dbert_pca_hstack_dict)\n",
    "similarity_dbert_dict_opp_app_vstack = topn_similar(opp__dbert_pca_vstack_dict, app__dbert_pca_vstack_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c.2.1 Saving the similarity data - distillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data \n",
    "save_processed_data(similarity_dbert_dict_opp_app_hstack, \"similarity_dict_dbert_hstack\")\n",
    "save_processed_data(cosine_similarity_dbert_opp_app_hstack, \"cosine_similarity_dbert_hstack\")\n",
    "\n",
    "# vstack\n",
    "save_processed_data(similarity_dbert_dict_opp_app_vstack, \"similarity_dict_dbert_vstack\")\n",
    "save_processed_data(cosine_similarity_dbert_opp_app_vstack, \"cosine_similarity_dbert_vstack\")\n",
    "\n",
    "#Saving the dimensionally reduced vectors for streamlit app output\n",
    "save_app_data(opp__dbert_pca_hstack_dict, 'opp__dbert_pca_hstack_dict')\n",
    "save_app_data(app__dbert_pca_hstack_dict, 'app__dbert_pca_hstack_dict')\n",
    "save_app_data(opp__dbert_pca_vstack_dict, 'opp__dbert_pca_vstack_dict')\n",
    "save_app_data(app__dbert_pca_vstack_dict, 'app__dbert_pca_vstack_dict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
